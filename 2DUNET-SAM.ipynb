{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f1f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "#model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "#    in_channels=1, out_channels=1, init_features=32, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f13361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m256149/Documents/LeftVentricleSegmentation/code/sam/segmentation_models_pytorch/encoders/sam.py:121: UserWarning: Only 153 out of pretrained 177 SAM image encoder modules are loaded. Missing modules: []. Unused modules: ['blocks.0.attn.rel_pos_h', 'blocks.0.attn.rel_pos_w', 'blocks.1.attn.rel_pos_h', 'blocks.1.attn.rel_pos_w', 'blocks.2.attn.rel_pos_h', 'blocks.2.attn.rel_pos_w', 'blocks.3.attn.rel_pos_h', 'blocks.3.attn.rel_pos_w', 'blocks.4.attn.rel_pos_h', 'blocks.4.attn.rel_pos_w', 'blocks.5.attn.rel_pos_h', 'blocks.5.attn.rel_pos_w', 'blocks.6.attn.rel_pos_h', 'blocks.6.attn.rel_pos_w', 'blocks.7.attn.rel_pos_h', 'blocks.7.attn.rel_pos_w', 'blocks.8.attn.rel_pos_h', 'blocks.8.attn.rel_pos_w', 'blocks.9.attn.rel_pos_h', 'blocks.9.attn.rel_pos_w', 'blocks.10.attn.rel_pos_h', 'blocks.10.attn.rel_pos_w', 'blocks.11.attn.rel_pos_h', 'blocks.11.attn.rel_pos_w'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install git+https://github.com/Rusteam/segmentation_models.pytorch.git@sam\n",
    "import segmentation_models_pytorch as smp\n",
    "model = smp.create_model(\"Unet\", \"sam-vit_b\", encoder_weights=\"sa-1b\", encoder_depth=4, decoder_channels=[256, 128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb14268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = list(model.children())\n",
    "#w = model[0].weight\n",
    "#model[0] = nn.Conv2d(1, 768, kernel_size=16, stride=16)\n",
    "#model[0].weight = nn.Parameter(torch.mean(w, dim=1, keepdim=True))\n",
    "#model = nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db2110bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce358cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import adabound\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix\n",
    "from torch.optim import Adam,SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99992ec",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d40564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainbatch(traindir,labdir,batch_size):\n",
    "    np.random.seed(0)\n",
    "    #np.random.shuffle(traindir)\n",
    "    return [(traindir[i:i + batch_size],labdir[i:i + batch_size]) for i in range(0, len(traindir), batch_size)]\n",
    "device = 'cuda:0'\n",
    "datadir = '/home/m256149/Documents/LeftVentricleSegmentation/data/Resources-4/'\n",
    "np.random.seed(0)\n",
    "for a,b,c in os.walk(datadir):\n",
    "    break\n",
    "tr = []\n",
    "lb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e6619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in b:\n",
    "    for q,fi,qw in os.walk(datadir+i+'/'):\n",
    "        break\n",
    "    for l in qw:\n",
    "        if 'frame' in l:\n",
    "            if 'gt' not in l:\n",
    "                tr.append(datadir+i+'/'+l)\n",
    "                lb.append(datadir+i+'/'+l[:-7]+'_gt'+'.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448c9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class diceloss(torch.nn.Module):\n",
    "    def init(self):\n",
    "        super(diceLoss, self).init()\n",
    "    def forward(self,pred, target):\n",
    "        smooth = 1.\n",
    "        iflat = pred.contiguous().view(-1)\n",
    "        tflat = target.contiguous().view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        A_sum = torch.sum(iflat * iflat)\n",
    "        B_sum = torch.sum(tflat * tflat)\n",
    "        return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        smooth = 1\n",
    "        num = targets.size(0)\n",
    "        \"\"\"\n",
    "               I am assuming the model does not have sigmoid layer in the end. if that is the case, change torch.sigmoid(logits) to simply logits\n",
    "        \"\"\"\n",
    "        probs = logits#torch.sigmoid(logits)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = targets.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "\n",
    "        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "        score = 1 - score.sum() / num\n",
    "        \n",
    "        \n",
    "        # adding TV regularization\n",
    "        tv_h = ((logits[:,:,1:,:] - logits[:,:,:-1,:]).pow(2)).sum()\n",
    "        tv_w = ((logits[:,:,:,1:] - logits[:,:,:,:-1]).pow(2)).sum()    \n",
    "       \n",
    "        return score #+ 0.001*(tv_h + tv_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb687f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t val loss : \t train loss :  1.0041501492261886\n",
      "Epoch :  2 \t val loss : \t train loss :  1.0688541400432587\n",
      "Epoch :  3 \t val loss : \t train loss :  0.9998055854439736\n",
      "Epoch :  4 \t val loss : \t train loss :  0.9997897490859031\n",
      "Epoch :  5 \t val loss : \t train loss :  0.9981610748171806\n",
      "Epoch :  6 \t val loss : \t train loss :  0.9741992220282555\n",
      "Epoch :  7 \t val loss : \t train loss :  0.9724324229359627\n",
      "Epoch :  8 \t val loss : \t train loss :  0.9590441966056824\n",
      "Epoch :  9 \t val loss : \t train loss :  0.954014353454113\n",
      "Epoch :  10 \t val loss : \t train loss :  0.9486674031615258\n",
      "Epoch :  11 \t val loss : \t train loss :  0.9457015436887741\n",
      "Epoch :  12 \t val loss : \t train loss :  0.9446301352977753\n",
      "Epoch :  13 \t val loss : \t train loss :  0.9429363602399826\n",
      "Epoch :  14 \t val loss : \t train loss :  0.9308651474118232\n",
      "Epoch :  15 \t val loss : \t train loss :  0.9220324686169624\n",
      "Epoch :  16 \t val loss : \t train loss :  0.9348658537864685\n",
      "Epoch :  17 \t val loss : \t train loss :  0.9269217988848686\n",
      "Epoch :  18 \t val loss : \t train loss :  0.9056089201569557\n",
      "Epoch :  19 \t val loss : \t train loss :  0.8985557848215103\n",
      "Epoch :  20 \t val loss : \t train loss :  0.882541897892952\n",
      "Epoch :  21 \t val loss : \t train loss :  0.8901561072468758\n",
      "Epoch :  22 \t val loss : \t train loss :  0.8768383967876434\n",
      "Epoch :  23 \t val loss : \t train loss :  0.8883562868833542\n",
      "Epoch :  24 \t val loss : \t train loss :  0.9999073314666748\n",
      "Epoch :  25 \t val loss : \t train loss :  0.9997960057854652\n",
      "Epoch :  26 \t val loss : \t train loss :  0.9994549939036369\n",
      "Epoch :  27 \t val loss : \t train loss :  0.9847789788246155\n",
      "Epoch :  28 \t val loss : \t train loss :  0.9725730881094933\n",
      "Epoch :  29 \t val loss : \t train loss :  0.9662873518466949\n",
      "Epoch :  30 \t val loss : \t train loss :  0.9605106341838837\n",
      "Epoch :  31 \t val loss : \t train loss :  0.9558057281374931\n",
      "Epoch :  32 \t val loss : \t train loss :  0.951057581603527\n",
      "Epoch :  33 \t val loss : \t train loss :  0.9452122220396996\n",
      "Epoch :  34 \t val loss : \t train loss :  0.9245961153507233\n",
      "Epoch :  35 \t val loss : \t train loss :  0.7835164022445679\n",
      "Epoch :  36 \t val loss : \t train loss :  0.736851889193058\n",
      "Epoch :  37 \t val loss : \t train loss :  0.9067771998047829\n",
      "Epoch :  38 \t val loss : \t train loss :  0.9647470888495445\n",
      "Epoch :  39 \t val loss : \t train loss :  0.9999363851547242\n",
      "Epoch :  40 \t val loss : \t train loss :  0.9999200150370597\n",
      "Epoch :  41 \t val loss : \t train loss :  0.9999022287130356\n",
      "Epoch :  42 \t val loss : \t train loss :  0.9998831737041474\n",
      "Epoch :  43 \t val loss : \t train loss :  0.9998633596301079\n",
      "Epoch :  44 \t val loss : \t train loss :  0.9998337435722351\n",
      "Epoch :  45 \t val loss : \t train loss :  0.9996874141693115\n",
      "Epoch :  46 \t val loss : \t train loss :  0.9977772319316864\n",
      "Epoch :  47 \t val loss : \t train loss :  0.987476686835289\n",
      "Epoch :  48 \t val loss : \t train loss :  0.9697934365272523\n",
      "Epoch :  49 \t val loss : \t train loss :  0.9363109561800956\n",
      "Epoch :  50 \t val loss : \t train loss :  0.9733899003267288\n",
      "Epoch :  51 \t val loss : \t train loss :  0.9471785578131676\n",
      "Epoch :  52 \t val loss : \t train loss :  0.9788106825947761\n",
      "Epoch :  53 \t val loss : \t train loss :  0.9988937088847161\n",
      "Epoch :  54 \t val loss : \t train loss :  0.9988515442609787\n",
      "Epoch :  55 \t val loss : \t train loss :  0.998793937265873\n",
      "Epoch :  56 \t val loss : \t train loss :  0.9982384735345841\n",
      "Epoch :  57 \t val loss : \t train loss :  0.9963837030529976\n",
      "Epoch :  58 \t val loss : \t train loss :  0.9929872006177902\n",
      "Epoch :  59 \t val loss : \t train loss :  0.9986739340424537\n",
      "Epoch :  60 \t val loss : \t train loss :  0.9979768165946007\n",
      "Epoch :  61 \t val loss : \t train loss :  0.9995470210909844\n",
      "Epoch :  62 \t val loss : \t train loss :  0.9994923320412635\n",
      "Epoch :  63 \t val loss : \t train loss :  0.9994368126988411\n",
      "Epoch :  64 \t val loss : \t train loss :  0.9955608227849007\n",
      "Epoch :  65 \t val loss : \t train loss :  0.960450817644596\n",
      "Epoch :  66 \t val loss : \t train loss :  0.926522102355957\n",
      "Epoch :  67 \t val loss : \t train loss :  0.999798800945282\n",
      "Epoch :  68 \t val loss : \t train loss :  0.9999870494008064\n",
      "Epoch :  69 \t val loss : \t train loss :  0.9996136862039566\n",
      "Epoch :  70 \t val loss : \t train loss :  0.9980872455239296\n",
      "Epoch :  71 \t val loss : \t train loss :  0.9993018534779549\n",
      "Epoch :  72 \t val loss : \t train loss :  0.9996781447529792\n",
      "Epoch :  73 \t val loss : \t train loss :  0.9989674845337868\n",
      "Epoch :  74 \t val loss : \t train loss :  1.000311915874481\n",
      "Epoch :  75 \t val loss : \t train loss :  0.9999714681506157\n",
      "Epoch :  76 \t val loss : \t train loss :  0.9991400906443596\n",
      "Epoch :  77 \t val loss : \t train loss :  0.9994974532723426\n",
      "Epoch :  78 \t val loss : \t train loss :  0.999453931748867\n",
      "Epoch :  79 \t val loss : \t train loss :  0.9994004827737808\n",
      "Epoch :  80 \t val loss : \t train loss :  0.9981149557232857\n",
      "Epoch :  81 \t val loss : \t train loss :  0.9955391302704811\n",
      "Epoch :  82 \t val loss : \t train loss :  0.999245867729187\n",
      "Epoch :  83 \t val loss : \t train loss :  0.9923376476764679\n",
      "Epoch :  84 \t val loss : \t train loss :  0.999173504114151\n",
      "Epoch :  85 \t val loss : \t train loss :  0.99908333837986\n",
      "Epoch :  86 \t val loss : \t train loss :  0.9989549461007118\n",
      "Epoch :  87 \t val loss : \t train loss :  0.9988423621654511\n",
      "Epoch :  88 \t val loss : \t train loss :  0.9986130276322365\n",
      "Epoch :  89 \t val loss : \t train loss :  0.9847895058989525\n",
      "Epoch :  90 \t val loss : \t train loss :  0.9939973545074463\n",
      "Epoch :  91 \t val loss : \t train loss :  1.0014905047416687\n",
      "Epoch :  92 \t val loss : \t train loss :  1.000786685347557\n",
      "Epoch :  93 \t val loss : \t train loss :  1.0004607605934144\n",
      "Epoch :  94 \t val loss : \t train loss :  1.0002851802110673\n",
      "Epoch :  95 \t val loss : \t train loss :  1.0001995497941971\n",
      "Epoch :  96 \t val loss : \t train loss :  1.0001214230060578\n",
      "Epoch :  97 \t val loss : \t train loss :  1.000045417547226\n",
      "Epoch :  98 \t val loss : \t train loss :  0.9999712628126144\n",
      "Epoch :  99 \t val loss : \t train loss :  0.9998927694559098\n",
      "Epoch :  100 \t val loss : \t train loss :  0.9997980600595474\n",
      "Epoch :  101 \t val loss : \t train loss :  0.9996880349516869\n",
      "Epoch :  102 \t val loss : \t train loss :  0.9995264872908592\n",
      "Epoch :  103 \t val loss : \t train loss :  0.9991547429561615\n",
      "Epoch :  104 \t val loss : \t train loss :  1.0002198833227158\n",
      "Epoch :  105 \t val loss : \t train loss :  1.0001212805509567\n",
      "Epoch :  106 \t val loss : \t train loss :  1.0000650292634965\n",
      "Epoch :  107 \t val loss : \t train loss :  1.0000255703926086\n",
      "Epoch :  108 \t val loss : \t train loss :  0.9999904868006706\n",
      "Epoch :  109 \t val loss : \t train loss :  0.9999471470713616\n",
      "Epoch :  110 \t val loss : \t train loss :  0.999888669848442\n",
      "Epoch :  111 \t val loss : \t train loss :  0.9998176443576813\n",
      "Epoch :  112 \t val loss : \t train loss :  0.9992026153206826\n",
      "Epoch :  113 \t val loss : \t train loss :  0.999769235253334\n",
      "Epoch :  114 \t val loss : \t train loss :  0.9995424249768257\n",
      "Epoch :  115 \t val loss : \t train loss :  0.9998926985263824\n",
      "Epoch :  116 \t val loss : \t train loss :  0.999009675681591\n",
      "Epoch :  117 \t val loss : \t train loss :  0.999768081009388\n",
      "Epoch :  118 \t val loss : \t train loss :  0.9994146174192429\n",
      "Epoch :  119 \t val loss : \t train loss :  0.9846496057510375\n",
      "Epoch :  120 \t val loss : \t train loss :  0.9801487016677857\n",
      "Epoch :  121 \t val loss : \t train loss :  0.9999319851398468\n",
      "Epoch :  122 \t val loss : \t train loss :  0.9999294659495354\n",
      "Epoch :  123 \t val loss : \t train loss :  0.9999266377091408\n",
      "Epoch :  124 \t val loss : \t train loss :  0.9999234449863433\n",
      "Epoch :  125 \t val loss : \t train loss :  0.9999206164479255\n",
      "Epoch :  126 \t val loss : \t train loss :  0.9999179777503013\n",
      "Epoch :  127 \t val loss : \t train loss :  0.9999155229330063\n",
      "Epoch :  128 \t val loss : \t train loss :  0.9999132072925567\n",
      "Epoch :  129 \t val loss : \t train loss :  0.9999107015132904\n",
      "Epoch :  130 \t val loss : \t train loss :  0.9999080374836922\n",
      "Epoch :  131 \t val loss : \t train loss :  0.9999052172899247\n",
      "Epoch :  132 \t val loss : \t train loss :  0.9999018841981888\n",
      "Epoch :  133 \t val loss : \t train loss :  0.9998981738090515\n",
      "Epoch :  134 \t val loss : \t train loss :  0.9998941376805306\n",
      "Epoch :  135 \t val loss : \t train loss :  0.9998893907666206\n",
      "Epoch :  136 \t val loss : \t train loss :  0.9998838800191879\n",
      "Epoch :  137 \t val loss : \t train loss :  0.9998769593238831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  138 \t val loss : \t train loss :  0.9998675549030304\n",
      "Epoch :  139 \t val loss : \t train loss :  0.9998561808466911\n",
      "Epoch :  140 \t val loss : \t train loss :  0.999843764603138\n",
      "Epoch :  141 \t val loss : \t train loss :  0.9995532649755478\n",
      "Epoch :  142 \t val loss : \t train loss :  0.9991085371375084\n",
      "Epoch :  143 \t val loss : \t train loss :  0.9989633059501648\n",
      "Epoch :  144 \t val loss : \t train loss :  0.9988516092300415\n",
      "Epoch :  145 \t val loss : \t train loss :  0.9987598884105683\n",
      "Epoch :  146 \t val loss : \t train loss :  0.9986681360006332\n",
      "Epoch :  147 \t val loss : \t train loss :  0.998558396100998\n",
      "Epoch :  148 \t val loss : \t train loss :  0.9984358912706375\n",
      "Epoch :  149 \t val loss : \t train loss :  0.9909715619683266\n",
      "Epoch :  150 \t val loss : \t train loss :  1.0002916908264161\n",
      "Epoch :  151 \t val loss : \t train loss :  0.999891360104084\n",
      "Epoch :  152 \t val loss : \t train loss :  0.9985039430856705\n",
      "Epoch :  153 \t val loss : \t train loss :  1.0193736293911935\n",
      "Epoch :  154 \t val loss : \t train loss :  1.000419871211052\n",
      "Epoch :  155 \t val loss : \t train loss :  1.0004144245386124\n",
      "Epoch :  156 \t val loss : \t train loss :  1.0004143780469894\n",
      "Epoch :  157 \t val loss : \t train loss :  1.000414316058159\n",
      "Epoch :  158 \t val loss : \t train loss :  1.0004142379760743\n",
      "Epoch :  159 \t val loss : \t train loss :  1.0004142105579377\n",
      "Epoch :  160 \t val loss : \t train loss :  1.0004141682386398\n",
      "Epoch :  161 \t val loss : \t train loss :  1.0004140746593475\n",
      "Epoch :  162 \t val loss : \t train loss :  1.000413988828659\n",
      "Epoch :  163 \t val loss : \t train loss :  1.0004138934612274\n",
      "Epoch :  164 \t val loss : \t train loss :  1.0004138213396072\n",
      "Epoch :  165 \t val loss : \t train loss :  1.0004137533903121\n",
      "Epoch :  166 \t val loss : \t train loss :  1.0004136836528779\n",
      "Epoch :  167 \t val loss : \t train loss :  1.0004135966300964\n",
      "Epoch :  168 \t val loss : \t train loss :  1.0004135024547578\n",
      "Epoch :  169 \t val loss : \t train loss :  1.0004134064912795\n",
      "Epoch :  170 \t val loss : \t train loss :  1.000413237810135\n",
      "Epoch :  171 \t val loss : \t train loss :  1.0004130786657333\n",
      "Epoch :  172 \t val loss : \t train loss :  1.0004129499197005\n",
      "Epoch :  173 \t val loss : \t train loss :  1.0004128032922746\n",
      "Epoch :  174 \t val loss : \t train loss :  1.0004126119613648\n",
      "Epoch :  175 \t val loss : \t train loss :  1.0004123955965043\n",
      "Epoch :  176 \t val loss : \t train loss :  1.0004121732711793\n",
      "Epoch :  177 \t val loss : \t train loss :  1.0004119098186492\n",
      "Epoch :  178 \t val loss : \t train loss :  1.000411605834961\n",
      "Epoch :  179 \t val loss : \t train loss :  1.0004113221168518\n",
      "Epoch :  180 \t val loss : \t train loss :  1.0004109877347946\n",
      "Epoch :  181 \t val loss : \t train loss :  1.0004105973243713\n",
      "Epoch :  182 \t val loss : \t train loss :  1.0004102331399918\n",
      "Epoch :  183 \t val loss : \t train loss :  1.0004097670316696\n",
      "Epoch :  184 \t val loss : \t train loss :  1.0004092478752136\n",
      "Epoch :  185 \t val loss : \t train loss :  1.0004087048768997\n",
      "Epoch :  186 \t val loss : \t train loss :  1.0004080498218537\n",
      "Epoch :  187 \t val loss : \t train loss :  1.0004074120521544\n",
      "Epoch :  188 \t val loss : \t train loss :  1.0004067170619964\n",
      "Epoch :  189 \t val loss : \t train loss :  1.0004058849811555\n",
      "Epoch :  190 \t val loss : \t train loss :  1.0004049986600876\n",
      "Epoch :  191 \t val loss : \t train loss :  1.0004040342569351\n",
      "Epoch :  192 \t val loss : \t train loss :  1.0004029095172882\n",
      "Epoch :  193 \t val loss : \t train loss :  1.000401703119278\n",
      "Epoch :  194 \t val loss : \t train loss :  1.0004003238677979\n",
      "Epoch :  195 \t val loss : \t train loss :  1.0003988254070282\n",
      "Epoch :  196 \t val loss : \t train loss :  1.000397201180458\n",
      "Epoch :  197 \t val loss : \t train loss :  1.0003954827785493\n",
      "Epoch :  198 \t val loss : \t train loss :  1.0003936725854874\n",
      "Epoch :  199 \t val loss : \t train loss :  1.0003917944431304\n",
      "Epoch :  200 \t val loss : \t train loss :  1.0003897857666015\n",
      "Epoch :  201 \t val loss : \t train loss :  1.0003877347707748\n",
      "Epoch :  202 \t val loss : \t train loss :  1.0003856641054154\n",
      "Epoch :  203 \t val loss : \t train loss :  1.000383472442627\n",
      "Epoch :  204 \t val loss : \t train loss :  1.0003812736272812\n",
      "Epoch :  205 \t val loss : \t train loss :  1.0003789657354354\n",
      "Epoch :  206 \t val loss : \t train loss :  1.0003764587640762\n",
      "Epoch :  207 \t val loss : \t train loss :  1.000373858809471\n",
      "Epoch :  208 \t val loss : \t train loss :  1.0003711020946502\n",
      "Epoch :  209 \t val loss : \t train loss :  1.000368058681488\n",
      "Epoch :  210 \t val loss : \t train loss :  1.0003647124767303\n",
      "Epoch :  211 \t val loss : \t train loss :  1.000361092686653\n",
      "Epoch :  212 \t val loss : \t train loss :  1.000357077717781\n",
      "Epoch :  213 \t val loss : \t train loss :  1.000352338552475\n",
      "Epoch :  214 \t val loss : \t train loss :  1.0003448659181595\n",
      "Epoch :  215 \t val loss : \t train loss :  1.0003382825851441\n",
      "Epoch :  216 \t val loss : \t train loss :  1.000333337187767\n",
      "Epoch :  217 \t val loss : \t train loss :  1.0003280621767043\n",
      "Epoch :  218 \t val loss : \t train loss :  1.0003221613168716\n",
      "Epoch :  219 \t val loss : \t train loss :  1.0003152924776078\n",
      "Epoch :  220 \t val loss : \t train loss :  1.000307268500328\n",
      "Epoch :  221 \t val loss : \t train loss :  1.00029898583889\n",
      "Epoch :  222 \t val loss : \t train loss :  1.0002901899814605\n",
      "Epoch :  223 \t val loss : \t train loss :  1.0002797418832778\n",
      "Epoch :  224 \t val loss : \t train loss :  1.0002611809968949\n",
      "Epoch :  225 \t val loss : \t train loss :  1.0002291631698608\n",
      "Epoch :  226 \t val loss : \t train loss :  1.0001921719312667\n",
      "Epoch :  227 \t val loss : \t train loss :  1.0001440078020096\n",
      "Epoch :  228 \t val loss : \t train loss :  0.9953506690263748\n",
      "Epoch :  229 \t val loss : \t train loss :  0.9807371053099633\n",
      "Epoch :  230 \t val loss : \t train loss :  0.9987147840857505\n",
      "Epoch :  231 \t val loss : \t train loss :  0.998542976975441\n",
      "Epoch :  232 \t val loss : \t train loss :  0.9983633324503899\n",
      "Epoch :  233 \t val loss : \t train loss :  0.9982428273558617\n",
      "Epoch :  234 \t val loss : \t train loss :  0.9980235576629639\n",
      "Epoch :  235 \t val loss : \t train loss :  0.9972570598125458\n",
      "Epoch :  236 \t val loss : \t train loss :  0.9988607773184777\n",
      "Epoch :  237 \t val loss : \t train loss :  0.9999301397800445\n",
      "Epoch :  238 \t val loss : \t train loss :  0.9998553055524826\n",
      "Epoch :  239 \t val loss : \t train loss :  0.999669539630413\n",
      "Epoch :  240 \t val loss : \t train loss :  1.0000522589683534\n",
      "Epoch :  241 \t val loss : \t train loss :  1.000044201016426\n",
      "Epoch :  242 \t val loss : \t train loss :  1.0000411933660507\n",
      "Epoch :  243 \t val loss : \t train loss :  1.0000386065244675\n",
      "Epoch :  244 \t val loss : \t train loss :  1.00003624856472\n",
      "Epoch :  245 \t val loss : \t train loss :  1.0000340837240218\n",
      "Epoch :  246 \t val loss : \t train loss :  1.0000321352481842\n",
      "Epoch :  247 \t val loss : \t train loss :  1.0000300472974777\n",
      "Epoch :  248 \t val loss : \t train loss :  1.000027900338173\n",
      "Epoch :  249 \t val loss : \t train loss :  1.0000256115198136\n",
      "Epoch :  250 \t val loss : \t train loss :  1.0000232201814652\n",
      "Epoch :  251 \t val loss : \t train loss :  1.0000204879045487\n",
      "Epoch :  252 \t val loss : \t train loss :  1.0000179278850556\n",
      "Epoch :  253 \t val loss : \t train loss :  1.0000163561105728\n",
      "Epoch :  254 \t val loss : \t train loss :  1.0000147068500518\n",
      "Epoch :  255 \t val loss : \t train loss :  1.000012845993042\n",
      "Epoch :  256 \t val loss : \t train loss :  1.0000106865167617\n",
      "Epoch :  257 \t val loss : \t train loss :  1.0000079864263534\n",
      "Epoch :  258 \t val loss : \t train loss :  1.0000041860342026\n",
      "Epoch :  259 \t val loss : \t train loss :  0.9999974438548088\n",
      "Epoch :  260 \t val loss : \t train loss :  0.9999817955493927\n",
      "Epoch :  261 \t val loss : \t train loss :  0.9999635609984397\n",
      "Epoch :  262 \t val loss : \t train loss :  0.9999274203181266\n",
      "Epoch :  263 \t val loss : \t train loss :  0.9993281397223472\n",
      "Epoch :  264 \t val loss : \t train loss :  0.9998613280057908\n",
      "Epoch :  265 \t val loss : \t train loss :  1.0000707554817199\n",
      "Epoch :  266 \t val loss : \t train loss :  1.0000139379501343\n",
      "Epoch :  267 \t val loss : \t train loss :  0.999587394297123\n",
      "Epoch :  268 \t val loss : \t train loss :  0.9996711909770966\n",
      "Epoch :  269 \t val loss : \t train loss :  1.000075531899929\n",
      "Epoch :  270 \t val loss : \t train loss :  0.9998503184318542\n",
      "Epoch :  271 \t val loss : \t train loss :  0.9993026360869408\n",
      "Epoch :  272 \t val loss : \t train loss :  0.9999994322657585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  273 \t val loss : \t train loss :  0.9999333497881889\n",
      "Epoch :  274 \t val loss : \t train loss :  0.9999108901619911\n",
      "Epoch :  275 \t val loss : \t train loss :  0.9998690712451935\n",
      "Epoch :  276 \t val loss : \t train loss :  0.9999930796027183\n",
      "Epoch :  277 \t val loss : \t train loss :  0.9996160167455673\n",
      "Epoch :  278 \t val loss : \t train loss :  0.9999848246574402\n",
      "Epoch :  279 \t val loss : \t train loss :  0.9999615204334259\n",
      "Epoch :  280 \t val loss : \t train loss :  0.9999544975161553\n",
      "Epoch :  281 \t val loss : \t train loss :  0.9999472188949585\n",
      "Epoch :  282 \t val loss : \t train loss :  0.9999354249238968\n",
      "Epoch :  283 \t val loss : \t train loss :  0.9998079219460487\n",
      "Epoch :  284 \t val loss : \t train loss :  1.000060768723488\n",
      "Epoch :  285 \t val loss : \t train loss :  0.9999079605937005\n",
      "Epoch :  286 \t val loss : \t train loss :  0.999546868801117\n",
      "Epoch :  287 \t val loss : \t train loss :  1.0000001123547555\n",
      "Epoch :  288 \t val loss : \t train loss :  0.9998498484492302\n",
      "Epoch :  289 \t val loss : \t train loss :  0.9993936309218406\n",
      "Epoch :  290 \t val loss : \t train loss :  0.9999744004011154\n",
      "Epoch :  291 \t val loss : \t train loss :  0.9999253791570664\n",
      "Epoch :  292 \t val loss : \t train loss :  0.9998597896099091\n",
      "Epoch :  293 \t val loss : \t train loss :  0.99850296407938\n",
      "Epoch :  294 \t val loss : \t train loss :  0.999979755282402\n",
      "Epoch :  295 \t val loss : \t train loss :  1.0000717413425446\n",
      "Epoch :  296 \t val loss : \t train loss :  1.0001452255249024\n",
      "Epoch :  297 \t val loss : \t train loss :  1.0001011395454407\n",
      "Epoch :  298 \t val loss : \t train loss :  1.000024543106556\n",
      "Epoch :  299 \t val loss : \t train loss :  1.000717662870884\n",
      "Epoch :  300 \t val loss : \t train loss :  0.9998347586393357\n",
      "Epoch :  301 \t val loss : \t train loss :  1.0051181450486184\n",
      "Epoch :  302 \t val loss : \t train loss :  1.0009812420606614\n",
      "Epoch :  303 \t val loss : \t train loss :  1.000976974964142\n",
      "Epoch :  304 \t val loss : \t train loss :  1.0009721994400025\n",
      "Epoch :  305 \t val loss : \t train loss :  1.000966705083847\n",
      "Epoch :  306 \t val loss : \t train loss :  1.0009600257873534\n",
      "Epoch :  307 \t val loss : \t train loss :  1.0009503090381622\n",
      "Epoch :  308 \t val loss : \t train loss :  1.000918802022934\n",
      "Epoch :  309 \t val loss : \t train loss :  1.0008682227134704\n",
      "Epoch :  310 \t val loss : \t train loss :  1.000811704993248\n",
      "Epoch :  311 \t val loss : \t train loss :  1.0007734847068788\n",
      "Epoch :  312 \t val loss : \t train loss :  1.000743706226349\n",
      "Epoch :  313 \t val loss : \t train loss :  1.0007151359319686\n",
      "Epoch :  314 \t val loss : \t train loss :  1.0006858575344086\n",
      "Epoch :  315 \t val loss : \t train loss :  1.000654485821724\n",
      "Epoch :  316 \t val loss : \t train loss :  1.0006207126379012\n",
      "Epoch :  317 \t val loss : \t train loss :  1.0005891615152358\n",
      "Epoch :  318 \t val loss : \t train loss :  1.0005507457256317\n",
      "Epoch :  319 \t val loss : \t train loss :  1.0005111914873124\n",
      "Epoch :  320 \t val loss : \t train loss :  1.00047119140625\n",
      "Epoch :  321 \t val loss : \t train loss :  1.000425934791565\n",
      "Epoch :  322 \t val loss : \t train loss :  1.000375663638115\n",
      "Epoch :  323 \t val loss : \t train loss :  1.0003077274560928\n",
      "Epoch :  324 \t val loss : \t train loss :  1.0001846307516098\n",
      "Epoch :  325 \t val loss : \t train loss :  1.0001180869340898\n",
      "Epoch :  326 \t val loss : \t train loss :  1.0001135355234145\n",
      "Epoch :  327 \t val loss : \t train loss :  1.000109140276909\n",
      "Epoch :  328 \t val loss : \t train loss :  1.000105100274086\n",
      "Epoch :  329 \t val loss : \t train loss :  1.0001011830568314\n",
      "Epoch :  330 \t val loss : \t train loss :  1.0000967812538146\n",
      "Epoch :  331 \t val loss : \t train loss :  1.0000923997163773\n",
      "Epoch :  332 \t val loss : \t train loss :  1.0000882411003114\n",
      "Epoch :  333 \t val loss : \t train loss :  1.0000838434696198\n",
      "Epoch :  334 \t val loss : \t train loss :  1.0000799536705016\n",
      "Epoch :  335 \t val loss : \t train loss :  1.0000763899087906\n",
      "Epoch :  336 \t val loss : \t train loss :  1.000072438120842\n",
      "Epoch :  337 \t val loss : \t train loss :  1.0000696432590486\n",
      "Epoch :  338 \t val loss : \t train loss :  1.000065225958824\n",
      "Epoch :  339 \t val loss : \t train loss :  1.0000622898340226\n",
      "Epoch :  340 \t val loss : \t train loss :  1.0000595933198928\n",
      "Epoch :  341 \t val loss : \t train loss :  1.0000568956136704\n",
      "Epoch :  342 \t val loss : \t train loss :  1.00005430996418\n",
      "Epoch :  343 \t val loss : \t train loss :  1.0000518947839736\n",
      "Epoch :  344 \t val loss : \t train loss :  1.0000492519140243\n",
      "Epoch :  345 \t val loss : \t train loss :  1.0000467908382416\n",
      "Epoch :  346 \t val loss : \t train loss :  1.0000444608926773\n",
      "Epoch :  347 \t val loss : \t train loss :  1.0000433468818664\n",
      "Epoch :  348 \t val loss : \t train loss :  1.000040010213852\n",
      "Epoch :  349 \t val loss : \t train loss :  1.0000379461050033\n",
      "Epoch :  350 \t val loss : \t train loss :  1.0000358760356902\n",
      "Epoch :  351 \t val loss : \t train loss :  1.0000340634584426\n",
      "Epoch :  352 \t val loss : \t train loss :  1.0000320774316789\n",
      "Epoch :  353 \t val loss : \t train loss :  1.0000303184986115\n",
      "Epoch :  354 \t val loss : \t train loss :  1.000028635263443\n",
      "Epoch :  355 \t val loss : \t train loss :  1.0000270342826842\n",
      "Epoch :  356 \t val loss : \t train loss :  1.0000255089998245\n",
      "Epoch :  357 \t val loss : \t train loss :  1.000024021267891\n",
      "Epoch :  358 \t val loss : \t train loss :  1.0000232148170471\n",
      "Epoch :  359 \t val loss : \t train loss :  1.0000214540958405\n",
      "Epoch :  360 \t val loss : \t train loss :  1.000020089149475\n",
      "Epoch :  361 \t val loss : \t train loss :  1.000018888115883\n",
      "Epoch :  362 \t val loss : \t train loss :  1.0000177592039108\n",
      "Epoch :  363 \t val loss : \t train loss :  1.0000166863203048\n",
      "Epoch :  364 \t val loss : \t train loss :  1.0000156199932098\n",
      "Epoch :  365 \t val loss : \t train loss :  1.0000146102905274\n",
      "Epoch :  366 \t val loss : \t train loss :  1.0000135755538941\n",
      "Epoch :  367 \t val loss : \t train loss :  1.0000125169754028\n",
      "Epoch :  368 \t val loss : \t train loss :  1.0000114315748214\n",
      "Epoch :  369 \t val loss : \t train loss :  1.0000102365016936\n",
      "Epoch :  370 \t val loss : \t train loss :  1.000009564757347\n",
      "Epoch :  371 \t val loss : \t train loss :  1.0000081688165665\n",
      "Epoch :  372 \t val loss : \t train loss :  1.0000060671567916\n",
      "Epoch :  373 \t val loss : \t train loss :  1.0000046133995055\n",
      "Epoch :  374 \t val loss : \t train loss :  1.00000326693058\n",
      "Epoch :  375 \t val loss : \t train loss :  1.0000019675493241\n",
      "Epoch :  376 \t val loss : \t train loss :  1.0000005981326103\n",
      "Epoch :  377 \t val loss : \t train loss :  0.999099622964859\n",
      "Epoch :  378 \t val loss : \t train loss :  0.99857596129179\n",
      "Epoch :  379 \t val loss : \t train loss :  0.9984581211209297\n",
      "Epoch :  380 \t val loss : \t train loss :  0.9983736497163772\n",
      "Epoch :  381 \t val loss : \t train loss :  0.9980627804994583\n",
      "Epoch :  382 \t val loss : \t train loss :  0.9990005627274513\n",
      "Epoch :  383 \t val loss : \t train loss :  1.0000419843196868\n",
      "Epoch :  384 \t val loss : \t train loss :  1.0000293838977814\n",
      "Epoch :  385 \t val loss : \t train loss :  1.000020837187767\n",
      "Epoch :  386 \t val loss : \t train loss :  1.000016154050827\n",
      "Epoch :  387 \t val loss : \t train loss :  1.0000123739242555\n",
      "Epoch :  388 \t val loss : \t train loss :  1.0000094002485276\n",
      "Epoch :  389 \t val loss : \t train loss :  1.000006383061409\n",
      "Epoch :  390 \t val loss : \t train loss :  1.000002788901329\n",
      "Epoch :  391 \t val loss : \t train loss :  0.99999791264534\n",
      "Epoch :  392 \t val loss : \t train loss :  0.9999917045235633\n",
      "Epoch :  393 \t val loss : \t train loss :  0.9999903893470764\n",
      "Epoch :  394 \t val loss : \t train loss :  0.9999896878004074\n",
      "Epoch :  395 \t val loss : \t train loss :  0.9999887290596962\n",
      "Epoch :  396 \t val loss : \t train loss :  0.999987296462059\n",
      "Epoch :  397 \t val loss : \t train loss :  0.999984410405159\n",
      "Epoch :  398 \t val loss : \t train loss :  0.9999612700939179\n",
      "Epoch :  399 \t val loss : \t train loss :  0.9981983861327172\n",
      "Epoch :  400 \t val loss : \t train loss :  0.9999050119519234\n",
      "Epoch :  401 \t val loss : \t train loss :  0.9999896469712257\n",
      "Epoch :  402 \t val loss : \t train loss :  1.0001286047697067\n",
      "Epoch :  403 \t val loss : \t train loss :  1.0001401889324188\n",
      "Epoch :  404 \t val loss : \t train loss :  0.9999712520837784\n",
      "Epoch :  405 \t val loss : \t train loss :  0.999529258608818\n",
      "Epoch :  406 \t val loss : \t train loss :  0.997377907037735\n",
      "Epoch :  407 \t val loss : \t train loss :  1.0018736624717712\n",
      "Epoch :  408 \t val loss : \t train loss :  1.0014897793531419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  409 \t val loss : \t train loss :  1.0012299436330796\n",
      "Epoch :  410 \t val loss : \t train loss :  1.001050528883934\n",
      "Epoch :  411 \t val loss : \t train loss :  1.000899893641472\n",
      "Epoch :  412 \t val loss : \t train loss :  1.0007381016016006\n",
      "Epoch :  413 \t val loss : \t train loss :  1.0005737006664277\n",
      "Epoch :  414 \t val loss : \t train loss :  1.000434119105339\n",
      "Epoch :  415 \t val loss : \t train loss :  1.0002961128950119\n",
      "Epoch :  416 \t val loss : \t train loss :  1.000174520611763\n",
      "Epoch :  417 \t val loss : \t train loss :  1.0001095843315124\n",
      "Epoch :  418 \t val loss : \t train loss :  1.0000771331787108\n",
      "Epoch :  419 \t val loss : \t train loss :  1.0000538820028304\n",
      "Epoch :  420 \t val loss : \t train loss :  1.0000305271148682\n",
      "Epoch :  421 \t val loss : \t train loss :  1.0000005039572715\n",
      "Epoch :  422 \t val loss : \t train loss :  0.9999632692337036\n",
      "Epoch :  423 \t val loss : \t train loss :  0.9999042493104935\n",
      "Epoch :  424 \t val loss : \t train loss :  0.9997094181180001\n",
      "Epoch :  425 \t val loss : \t train loss :  1.0003658431768416\n",
      "Epoch :  426 \t val loss : \t train loss :  0.9998862159252166\n",
      "Epoch :  427 \t val loss : \t train loss :  0.9998814180493355\n",
      "Epoch :  428 \t val loss : \t train loss :  0.9998742377758026\n",
      "Epoch :  429 \t val loss : \t train loss :  0.9998622018098832\n",
      "Epoch :  430 \t val loss : \t train loss :  0.9998466044664382\n",
      "Epoch :  431 \t val loss : \t train loss :  0.9998298615217209\n",
      "Epoch :  432 \t val loss : \t train loss :  0.9998095753788948\n",
      "Epoch :  433 \t val loss : \t train loss :  0.9997804301977158\n",
      "Epoch :  434 \t val loss : \t train loss :  0.9997263914346695\n",
      "Epoch :  435 \t val loss : \t train loss :  1.000009028017521\n",
      "Epoch :  436 \t val loss : \t train loss :  1.000136883854866\n",
      "Epoch :  437 \t val loss : \t train loss :  1.0001093178987503\n",
      "Epoch :  438 \t val loss : \t train loss :  1.0000804007053374\n",
      "Epoch :  439 \t val loss : \t train loss :  1.0000518703460692\n",
      "Epoch :  440 \t val loss : \t train loss :  1.0000254982709884\n",
      "Epoch :  441 \t val loss : \t train loss :  0.9999996173381805\n",
      "Epoch :  442 \t val loss : \t train loss :  0.9999724689126015\n",
      "Epoch :  443 \t val loss : \t train loss :  0.9999530309438706\n",
      "Epoch :  444 \t val loss : \t train loss :  0.9999464672803878\n",
      "Epoch :  445 \t val loss : \t train loss :  0.9999397963285446\n",
      "Epoch :  446 \t val loss : \t train loss :  0.9999319437146187\n",
      "Epoch :  447 \t val loss : \t train loss :  0.9999226287007332\n",
      "Epoch :  448 \t val loss : \t train loss :  0.9999110636115074\n",
      "Epoch :  449 \t val loss : \t train loss :  0.9998965156078339\n",
      "Epoch :  450 \t val loss : \t train loss :  0.9998801618814468\n",
      "Epoch :  451 \t val loss : \t train loss :  0.9998637950420379\n",
      "Epoch :  452 \t val loss : \t train loss :  0.9998435002565383\n",
      "Epoch :  453 \t val loss : \t train loss :  0.9998144766688347\n",
      "Epoch :  454 \t val loss : \t train loss :  0.9997722101211548\n",
      "Epoch :  455 \t val loss : \t train loss :  0.9996414360404015\n",
      "Epoch :  456 \t val loss : \t train loss :  0.9988644874095917\n",
      "Epoch :  457 \t val loss : \t train loss :  0.9999248167872429\n",
      "Epoch :  458 \t val loss : \t train loss :  0.9997890049219131\n",
      "Epoch :  459 \t val loss : \t train loss :  0.9997013965249062\n",
      "Epoch :  460 \t val loss : \t train loss :  0.9991274937987328\n",
      "Epoch :  461 \t val loss : \t train loss :  0.9988039979338645\n",
      "Epoch :  462 \t val loss : \t train loss :  1.0001174211502075\n",
      "Epoch :  463 \t val loss : \t train loss :  1.0000595313310623\n",
      "Epoch :  464 \t val loss : \t train loss :  1.0000334072113037\n",
      "Epoch :  465 \t val loss : \t train loss :  0.9999706533551216\n",
      "Epoch :  466 \t val loss : \t train loss :  0.9906807056069374\n",
      "Epoch :  467 \t val loss : \t train loss :  0.9993631449341774\n",
      "Epoch :  468 \t val loss : \t train loss :  0.99904647231102\n",
      "Epoch :  469 \t val loss : \t train loss :  0.9989898809790612\n",
      "Epoch :  470 \t val loss : \t train loss :  0.9995448142290115\n",
      "Epoch :  471 \t val loss : \t train loss :  1.0002362418174744\n",
      "Epoch :  472 \t val loss : \t train loss :  1.0000810220837593\n",
      "Epoch :  473 \t val loss : \t train loss :  0.9999190345406532\n",
      "Epoch :  474 \t val loss : \t train loss :  0.9997094884514809\n",
      "Epoch :  475 \t val loss : \t train loss :  0.9992502829432488\n",
      "Epoch :  476 \t val loss : \t train loss :  1.002316788136959\n",
      "Epoch :  477 \t val loss : \t train loss :  0.9997265729308128\n",
      "Epoch :  478 \t val loss : \t train loss :  0.99968571215868\n",
      "Epoch :  479 \t val loss : \t train loss :  0.9996624493598938\n",
      "Epoch :  480 \t val loss : \t train loss :  0.9996438720822334\n",
      "Epoch :  481 \t val loss : \t train loss :  0.9996205455064774\n",
      "Epoch :  482 \t val loss : \t train loss :  0.9995896565914154\n",
      "Epoch :  483 \t val loss : \t train loss :  0.9995454210042953\n",
      "Epoch :  484 \t val loss : \t train loss :  0.9994686445593834\n",
      "Epoch :  485 \t val loss : \t train loss :  0.9992469653487206\n",
      "Epoch :  486 \t val loss : \t train loss :  0.9996456345915794\n",
      "Epoch :  487 \t val loss : \t train loss :  0.9994886642694474\n",
      "Epoch :  488 \t val loss : \t train loss :  0.9989216431975365\n",
      "Epoch :  489 \t val loss : \t train loss :  0.9994342923164368\n",
      "Epoch :  490 \t val loss : \t train loss :  0.9993566611409187\n",
      "Epoch :  491 \t val loss : \t train loss :  0.9992124477028846\n",
      "Epoch :  492 \t val loss : \t train loss :  0.9992728972434998\n",
      "Epoch :  493 \t val loss : \t train loss :  1.0006342732906341\n",
      "Epoch :  494 \t val loss : \t train loss :  1.000457620024681\n",
      "Epoch :  495 \t val loss : \t train loss :  1.0003336322307588\n",
      "Epoch :  496 \t val loss : \t train loss :  1.00023395717144\n",
      "Epoch :  497 \t val loss : \t train loss :  1.0001579797267914\n",
      "Epoch :  498 \t val loss : \t train loss :  1.000077930688858\n",
      "Epoch :  499 \t val loss : \t train loss :  1.0000226348638535\n",
      "Epoch :  500 \t val loss : \t train loss :  1.0000106018781663\n",
      "Epoch :  501 \t val loss : \t train loss :  0.9999996483325958\n",
      "Epoch :  502 \t val loss : \t train loss :  0.9999876308441162\n",
      "Epoch :  503 \t val loss : \t train loss :  0.9999742156267166\n",
      "Epoch :  504 \t val loss : \t train loss :  0.99995884090662\n",
      "Epoch :  505 \t val loss : \t train loss :  0.999941349029541\n",
      "Epoch :  506 \t val loss : \t train loss :  0.9999189808964729\n",
      "Epoch :  507 \t val loss : \t train loss :  0.9998524469137192\n",
      "Epoch :  508 \t val loss : \t train loss :  0.9995238032937049\n",
      "Epoch :  509 \t val loss : \t train loss :  0.9996520102024078\n",
      "Epoch :  510 \t val loss : \t train loss :  1.0001568400859833\n",
      "Epoch :  511 \t val loss : \t train loss :  0.9999573025107383\n",
      "Epoch :  512 \t val loss : \t train loss :  0.9998307114839554\n",
      "Epoch :  513 \t val loss : \t train loss :  0.9997032949328423\n",
      "Epoch :  514 \t val loss : \t train loss :  0.9995501047372818\n",
      "Epoch :  515 \t val loss : \t train loss :  0.9991246435046196\n",
      "Epoch :  516 \t val loss : \t train loss :  0.9938539701700211\n",
      "Epoch :  517 \t val loss : \t train loss :  1.0000237599015236\n",
      "Epoch :  518 \t val loss : \t train loss :  0.9999380388855934\n",
      "Epoch :  519 \t val loss : \t train loss :  0.9998695591092109\n",
      "Epoch :  520 \t val loss : \t train loss :  0.9997770375013352\n",
      "Epoch :  521 \t val loss : \t train loss :  0.9994361579418183\n",
      "Epoch :  522 \t val loss : \t train loss :  0.9999129325151443\n",
      "Epoch :  523 \t val loss : \t train loss :  0.9998886531591415\n",
      "Epoch :  524 \t val loss : \t train loss :  0.9929910802841186\n",
      "Epoch :  525 \t val loss : \t train loss :  1.0014873081445694\n",
      "Epoch :  526 \t val loss : \t train loss :  1.000505723953247\n",
      "Epoch :  527 \t val loss : \t train loss :  1.0000119131803513\n",
      "Epoch :  528 \t val loss : \t train loss :  0.9997590529918671\n",
      "Epoch :  529 \t val loss : \t train loss :  0.9996021178364753\n",
      "Epoch :  530 \t val loss : \t train loss :  0.9994712632894516\n",
      "Epoch :  531 \t val loss : \t train loss :  0.9993608298897744\n",
      "Epoch :  532 \t val loss : \t train loss :  0.9993135213851929\n",
      "Epoch :  533 \t val loss : \t train loss :  0.9992789560556412\n",
      "Epoch :  534 \t val loss : \t train loss :  0.9989150187373161\n",
      "Epoch :  535 \t val loss : \t train loss :  1.0021952638030052\n",
      "Epoch :  536 \t val loss : \t train loss :  1.0006540763378142\n",
      "Epoch :  537 \t val loss : \t train loss :  1.0004521268606186\n",
      "Epoch :  538 \t val loss : \t train loss :  1.0003347760438919\n",
      "Epoch :  539 \t val loss : \t train loss :  1.0002901005744933\n",
      "Epoch :  540 \t val loss : \t train loss :  1.0002552109956742\n",
      "Epoch :  541 \t val loss : \t train loss :  1.0002237141132355\n",
      "Epoch :  542 \t val loss : \t train loss :  1.0001958918571472\n",
      "Epoch :  543 \t val loss : \t train loss :  1.000171844959259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  544 \t val loss : \t train loss :  1.0001502895355225\n",
      "Epoch :  545 \t val loss : \t train loss :  1.000132001042366\n",
      "Epoch :  546 \t val loss : \t train loss :  1.0001162278652191\n",
      "Epoch :  547 \t val loss : \t train loss :  1.0001021260023117\n",
      "Epoch :  548 \t val loss : \t train loss :  1.0000895392894744\n",
      "Epoch :  549 \t val loss : \t train loss :  1.00007807970047\n",
      "Epoch :  550 \t val loss : \t train loss :  1.0000678688287734\n",
      "Epoch :  551 \t val loss : \t train loss :  1.0000587892532349\n",
      "Epoch :  552 \t val loss : \t train loss :  1.0000504672527313\n",
      "Epoch :  553 \t val loss : \t train loss :  1.0000422435998917\n",
      "Epoch :  554 \t val loss : \t train loss :  1.000034448504448\n",
      "Epoch :  555 \t val loss : \t train loss :  1.0000267183780671\n",
      "Epoch :  556 \t val loss : \t train loss :  1.0000197333097458\n",
      "Epoch :  557 \t val loss : \t train loss :  1.0000137984752655\n",
      "Epoch :  558 \t val loss : \t train loss :  1.000008028447628\n",
      "Epoch :  559 \t val loss : \t train loss :  1.0000022372603417\n"
     ]
    }
   ],
   "source": [
    "train = tr\n",
    "num_epochs = 1000\n",
    "batch_size = 1\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#optimizer = adabound.AdaBound(vgg3d.parameters(), lr=0.4,weight_decay = 0.004,eps = 1e-4, final_lr=0.01)\n",
    "#optimizer = SGD(model.parameters(), lr=.01,momentum=0.9)#,momentum=0.9,weight_decay=0.0005)#SGD(vgg3d.parameters(), lr=0.01,momentum=0.9,weight_decay=0.0005)\n",
    "optimizer = Adam(model.parameters(), lr=.001)\n",
    "criterion = SoftDiceLoss()#diceloss()#nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss() ### momentum=0.9 diceloss()#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda(device = device)\n",
    "    criterion = criterion.cuda(device = device)\n",
    "val_losses = []\n",
    "train_losses_ep = []\n",
    "trans = tio.CropOrPad((3,1024,1024))\n",
    "transout = tio.CropOrPad((1,1024,1024))\n",
    "\n",
    "tRESCALE = tio.RescaleIntensity((-1,1))\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    for batch,lab in get_trainbatch(train,lb,int(batch_size)):\n",
    "        y_train = []\n",
    "        for cnt,i in enumerate(batch):\n",
    "            if cnt == 0:     \n",
    "                orig = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "                y_train = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) ==2\n",
    "                \n",
    "                orig = tRESCALE(trans(orig))\n",
    "                y_train = transout(y_train)\n",
    "            else:\n",
    "                temp = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "                temp_lab = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) == 2\n",
    "                \n",
    "                temp = tRESCALE(trans(temp))\n",
    "                temp_lab = transout(temp_lab)\n",
    "                \n",
    "                #print(temp_lab.shape,temp.shape,orig.shape,y_train.shape)\n",
    "                y_train = torch.cat((y_train,temp_lab),dim = 0)\n",
    "                orig = torch.cat((orig,temp),dim = 0)\n",
    "        \n",
    "        \n",
    "        orig = orig.float().cuda(device = device)\n",
    "        y_train = y_train.float().cuda(device = device)\n",
    "        \n",
    "        temp_num = np.random.choice(orig.shape[0],2,replace = False)\n",
    "        \n",
    "        orig = orig[temp_num,:,:,:]\n",
    "        y_train = y_train[temp_num,:,:,:]\n",
    "        \n",
    "        \n",
    "        # clearing the Gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_train = model(orig)\n",
    "        loss_train = criterion(output_train, y_train) \n",
    "        # computing the updated weights of all the model parameters\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss_train.item())\n",
    "        del orig\n",
    "        del y_train\n",
    "    train_losses_ep.append(np.mean(train_losses))\n",
    "    #if epoch%(num_epochs/25) == 0:\n",
    "        # printing the validation loss\n",
    "    print('Epoch : ',epoch+1, '\\t', 'val loss :', '\\t', 'train loss : ', np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/m256149/Documents/LeftVentricleSegmentation/code/unet-sam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    plt.figure(i)\n",
    "    \n",
    "    plt.imshow(model(orig).cpu().detach().numpy()[i,0,:,:])\n",
    "    plt.figure(i+100)\n",
    "    plt.imshow(y_train.cpu()[i,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5288fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(model(orig) ==1)).sum() , sum(sum(model(orig) ==0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(orig[0,:,:,:][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch,lab in get_trainbatch(train,lb,int(batch_size)):\n",
    "    y_train = []\n",
    "    for cnt,i in enumerate(batch):\n",
    "        if cnt == 0:     \n",
    "            orig = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            y_train = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) ==2\n",
    "\n",
    "            orig = tRESCALE(trans(orig))\n",
    "            y_train = trans(y_train)\n",
    "        else:\n",
    "            temp = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            temp_lab = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) == 2\n",
    "\n",
    "            temp = tRESCALE(trans(temp))\n",
    "            temp_lab = trans(temp_lab)\n",
    "\n",
    "            #print(temp_lab.shape,temp.shape,orig.shape,y_train.shape)\n",
    "            y_train = torch.cat((y_train,temp_lab),dim = 0)\n",
    "            orig = torch.cat((orig,temp),dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = orig.float().cuda(device = device)\n",
    "y_train = y_train.float().cuda(device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    plt.figure(i)\n",
    "    \n",
    "    plt.imshow(model(orig).cpu().detach().numpy()[i,0,:,:])\n",
    "    plt.figure(i+100)\n",
    "    plt.imshow(y_train.cpu()[i,0,:,:])\n",
    "    \n",
    "    plt.figure(i+200)\n",
    "    plt.imshow(orig.cpu().detach().numpy()[i,0,:,:],vmin = -1,vmax = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a061d",
   "metadata": {},
   "source": [
    "## Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d878704",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/home/m256149/Documents/LeftVentricleSegmentation/Resources-5/'\n",
    "#fat,water,study\n",
    "for a,b,c in os.walk(datadir):\n",
    "    break\n",
    "tr = []\n",
    "lb = []\n",
    "for i in b:\n",
    "    for q,fi,qw in os.walk(datadir+i+'/'):\n",
    "        break\n",
    "    for l in qw:\n",
    "        if 'frame' in l:\n",
    "            if 'gt' not in l:\n",
    "                tr.append(datadir+i+'/'+l)\n",
    "                lb.append(datadir+i+'/'+l[:-7]+'_gt'+'.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af494c",
   "metadata": {},
   "source": [
    "## calculate dice score on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10afdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ds = 0\n",
    "num_slice = 0 \n",
    "batch_size = 1\n",
    "for batch,lab in get_trainbatch(tr,lb,int(batch_size)):\n",
    "    y_test = []\n",
    "    for cnt,i in enumerate(batch):\n",
    "        if cnt == 0:     \n",
    "            orig = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            y_test = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) ==2\n",
    "\n",
    "            orig = tRESCALE(trans(orig))\n",
    "            y_test = trans(y_test)\n",
    "        else:\n",
    "            temp = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            temp_lab = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) == 2\n",
    "\n",
    "            temp = tRESCALE(trans(temp))\n",
    "            temp_lab = trans(temp_lab)\n",
    "\n",
    "            #print(temp_lab.shape,temp.shape,orig.shape,y_train.shape)\n",
    "            y_test = torch.cat((y_test,temp_lab),dim = 0)\n",
    "            orig = torch.cat((orig,temp),dim = 0)\n",
    "\n",
    "\n",
    "    orig = orig.float().cuda(device = device)\n",
    "    y_test = y_test.float().cuda(device = device)\n",
    "    ds += criterion(torch.round(model(orig)),y_test)*y_test.shape[0]\n",
    "    num_slice += y_test.shape[0]\n",
    "    del orig\n",
    "    del y_test\n",
    "print('average dice score', ds/num_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c621477",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/m256149/Documents/LeftVentricleSegmentation/unet-sam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=1, out_channels=1, init_features=32, pretrained=False)\n",
    "trainedmodel.load_state_dict(torch.load('/home/m256149/Documents/LeftVentricleSegmentation/unet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedmodel.eval()\n",
    "ds = 0\n",
    "num_slice = 0 \n",
    "batch_size = 8\n",
    "for batch,lab in get_trainbatch(tr,lb,int(batch_size)):\n",
    "    y_test = []\n",
    "    for cnt,i in enumerate(batch):\n",
    "        if cnt == 0:     \n",
    "            orig = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            y_test = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) ==2\n",
    "\n",
    "            orig = tRESCALE(trans(orig))\n",
    "            y_test = trans(y_test)\n",
    "        else:\n",
    "            temp = tio.ScalarImage(batch[cnt]).data.permute(3,0,1,2)\n",
    "            temp_lab = tio.ScalarImage(lab[cnt]).data.permute(3,0,1,2) == 2\n",
    "\n",
    "            temp = tRESCALE(trans(temp))\n",
    "            temp_lab = trans(temp_lab)\n",
    "\n",
    "            #print(temp_lab.shape,temp.shape,orig.shape,y_train.shape)\n",
    "            y_test = torch.cat((y_test,temp_lab),dim = 0)\n",
    "            orig = torch.cat((orig,temp),dim = 0)\n",
    "\n",
    "\n",
    "    orig = orig.float()#.cuda(device = device)\n",
    "    y_test = y_test.float()#.cuda(device = device)\n",
    "    ds += criterion(torch.round(trainedmodel(orig)),y_test).item()*y_test.shape[0]\n",
    "    num_slice += y_test.shape[0]\n",
    "    print(y_test.shape[0])\n",
    "    del orig\n",
    "    del y_test\n",
    "print('average dice score', ds/num_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07473251",
   "metadata": {},
   "source": [
    "## save masks as nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e701e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedmodel.eval()\n",
    "i = 0\n",
    "for batch,lab in zip(tr,lb):\n",
    "    orig = tio.ScalarImage(batch).data.permute(3,0,1,2)\n",
    "    orig = tRESCALE(trans(orig))\n",
    "\n",
    "    orig = orig.float()#.cuda(device = device)\n",
    "\n",
    "    maskdata = torch.round(trainedmodel(orig)).permute(1,2,3,0).detach()\n",
    "    \n",
    "   \n",
    "    y_test = tio.ScalarImage(lab).data.permute(3,0,1,2) ==2\n",
    "    y_test = trans(y_test)\n",
    "    \n",
    "    \n",
    "    # save orig, maskdata, y_test\n",
    "    temp = tio.ScalarImage(batch)\n",
    "    temp.data = orig.permute(1,2,3,0).detach()\n",
    "    temp.save(batch[:-14] + batch[-9:-7] +'resize.nii.gz')\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp = tio.ScalarImage(lab)\n",
    "    temp.data = maskdata\n",
    "    temp.save(lab[:-10] + '_outgt.nii.gz')\n",
    "    \n",
    "    temp = tio.ScalarImage(lab)\n",
    "    temp.data = y_test.permute(1,2,3,0).detach()\n",
    "    temp.save(lab[:-10] + '_resizegt.nii.gz')\n",
    "\n",
    "\n",
    "    \n",
    "    del orig\n",
    "    del y_test\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54ae75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
